Starting Training Loop...
Epoch 1/10
Validation Loss: 4.8730, Validation BLEU: 0.0000, Validation METEOR: 0.0000
0m 4s (- 0m 0s) (50 100%) 0.0877
Epoch 2/10
Validation Loss: 3.0836, Validation BLEU: 0.0000, Validation METEOR: 0.0000
0m 7s (- 0m 0s) (50 100%) 0.0655
Epoch 3/10
Validation Loss: 2.4346, Validation BLEU: 0.0000, Validation METEOR: 0.0313
0m 10s (- 0m 0s) (50 100%) 0.0556
Epoch 4/10
C:\Users\LAURA\AppData\Local\Programs\Python\Python39\lib\site-packages\nltk\translate\bleu_score.py:552: UserWarning:
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
C:\Users\LAURA\AppData\Local\Programs\Python\Python39\lib\site-packages\nltk\translate\bleu_score.py:552: UserWarning:
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
C:\Users\LAURA\AppData\Local\Programs\Python\Python39\lib\site-packages\nltk\translate\bleu_score.py:552: UserWarning:
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
Validation Loss: 2.1648, Validation BLEU: 0.0000, Validation METEOR: 0.0556
0m 13s (- 0m 0s) (50 100%) 0.0529
Epoch 5/10
Validation Loss: 1.7484, Validation BLEU: 0.0000, Validation METEOR: 0.0269
0m 17s (- 0m 0s) (50 100%) 0.0572
Epoch 6/10
Traceback (most recent call last):
  File "c:\Users\LAURA\Deep Learning\XNAPproject-grup-04\trainMaquina.py", line 258, in <module>
    main()
  File "c:\Users\LAURA\Deep Learning\XNAPproject-grup-04\trainMaquina.py", line 255, in main
    trainIters(encoder1, attn_decoder1, int(args.epochs), train_pairs, val_pairs, print_every=5000, learning_rate=float(args.lr))
  File "c:\Users\LAURA\Deep Learning\XNAPproject-grup-04\trainMaquina.py", line 186, in trainIters
    loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)
  File "c:\Users\LAURA\Deep Learning\XNAPproject-grup-04\trainMaquina.py", line 87, in train
    loss.backward()
  File "C:\Users\LAURA\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "C:\Users\LAURA\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\autograd\__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt